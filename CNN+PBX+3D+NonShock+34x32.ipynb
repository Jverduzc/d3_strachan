{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vHzMJfDhJ5I8it5pGiq4Y6W6BRhji6Pg",
      "authorship_tag": "ABX9TyMwG0IDomK8bx6QSwbqeLpC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jverduzc/d3_strachan/blob/master/CNN%2BPBX%2B3D%2BNonShock%2B34x32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5Nd2jeT8r8uu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import initializers\n",
        "from keras.layers import Input, Dropout, BatchNormalization, Conv3DTranspose, concatenate, Dense, Conv3D, Flatten, MaxPooling3D\n",
        "from keras.models import Sequential\n",
        "from sklearn.base import BaseEstimator\n",
        "import keras.backend as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "tf.keras.utils.set_random_seed(0)\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import sys\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "sys.path.insert(0, '../src/')\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### LOADING DATA\n",
        "\n",
        "def data_loader_inputs(file_list):\n",
        "\n",
        "  file_save = \"Inputs_\" + file_list[0]\n",
        "  file_path = file_list[1]\n",
        "\n",
        "  HE_densities_all_slices = []\n",
        "  Total_densities_all_slices = []\n",
        "  gb_all_slices = []\n",
        "\n",
        "  for i in range(1,17):\n",
        "    \n",
        "    # HEden\n",
        "\n",
        "    file_name = file_path + '/HEdenTable_slice' + str(i) + '.csv'\n",
        "    density_data = np.genfromtxt(file_name, delimiter=',', encoding=\"utf8\")\n",
        "    HE_densities_all_slices.append(density_data)\n",
        "    #print(density_data.shape)\n",
        "\n",
        "    # TotalDen\n",
        "\n",
        "    file_name = file_path + '/TotalDenTable_slice' + str(i) + '.csv'\n",
        "    tot_density_data = np.genfromtxt(file_name, delimiter=',', encoding=\"utf8\")\n",
        "    Total_densities_all_slices.append(tot_density_data)  \n",
        "    #print(tot_density_data.shape)\n",
        "\n",
        "    # GB\n",
        "\n",
        "    file_name = file_path + '/GBtable_slice' + str(i) + '.csv'\n",
        "    gb_data = np.genfromtxt(file_name, delimiter=',', encoding=\"utf8\")\n",
        "    gb_data = np.delete(gb_data, -1, 1)\n",
        "    gb_data[np.isnan(gb_data)] = 0    \n",
        "    gb_all_slices.append(gb_data)\n",
        "    #print(gb_data.shape)\n",
        "\n",
        "    \n",
        "  HE_densities_all_slices = np.array(HE_densities_all_slices).reshape(-1,34,32,1)\n",
        "  Total_densities_all_slices = np.array(Total_densities_all_slices).reshape(-1,34,32,1)\n",
        "  gb_all_slices = np.array(gb_all_slices).reshape(-1,34,32,1)\n",
        "\n",
        "  #print(HE_densities_all_slices.shape)\n",
        "  #print(Total_densities_all_slices.shape)\n",
        "  #print(gb_all_slices.shape)\n",
        "\n",
        "\n",
        "  ### PUTTING INPUTS TOGETHER\n",
        "\n",
        "  pbx_inputs = np.concatenate((HE_densities_all_slices, Total_densities_all_slices, gb_all_slices), axis=3)\n",
        "  #print(pbx_inputs.shape)\n",
        "\n",
        "  #### EXTENDING INPUTS\n",
        "\n",
        "  pbx_inputs = pbx_inputs.reshape(16,34,32,3)  \n",
        "  np.save(file_save, pbx_inputs)\n",
        "\n",
        "  return pbx_inputs\n"
      ],
      "metadata": {
        "id": "VbP3_kaVlGJ3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LOADING DATA / OUTPUTS\n",
        "\n",
        "def data_loader_outputs(file_list):\n",
        "\n",
        "  ### LOADING TEMPERATURES\n",
        "  file_save = \"TemperatureMap_\" + file_list[0]\n",
        "  file_path = file_list[1]\n",
        "\n",
        "  temperatures_all_slices = []\n",
        "  \n",
        "  for i in range(1,17):\n",
        "    \n",
        "    file_name = file_path + '/TempTable_slice' + str(i) + '.csv'\n",
        "    temp_data = np.genfromtxt(file_name, delimiter=',', encoding=\"utf8\")\n",
        "    # temp_data[np.isnan(temp_data)] = 0\n",
        "    # temp_data = np.delete(temp_data, -1, 1)\n",
        "    temp_data = temp_data/1000\n",
        "    #print(temp_data.shape)\n",
        "    temperatures_all_slices.append(temp_data)\n",
        "\n",
        "  temperatures_all_slices= np.array(temperatures_all_slices)\n",
        "  #print(temperatures_all_slices.shape)  \n",
        "\n",
        "  #### EXTENDING OUTPUTS\n",
        "\n",
        "  temperatures_all_slices = temperatures_all_slices.reshape(16,32,32,1)\n",
        "  np.save(file_save, temperatures_all_slices)\n",
        "\n",
        "  return temperatures_all_slices"
      ],
      "metadata": {
        "id": "PdTM91ptmavf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING DATA\n",
        "\n",
        "paths = [[\"PBX_PS14,7_Spherical_Oriented_Bottom.npy\"           , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX1/bin2nm_doublepad/btm'],\n",
        "         [\"PBX_PS14,7_Spherical_Oriented_Top.npy\"              , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX1/bin2nm_doublepad/top'],\n",
        "         [\"PBX_PS14,7_Spherical_Oriented_Bottom_Mirrored.npy\"  , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX1/bin2nm_doublepad_mirror/btm'],\n",
        "         [\"PBX_PS14,7_Spherical_Oriented_Top_Mirrored.npy\"     , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX1/bin2nm_doublepad_mirror/top'],\n",
        "         \n",
        "         [\"PBX_PS9,5_Spherical_Oriented_Bottom.npy\"           , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX2/bin2nm_doublepad/btm'],\n",
        "         [\"PBX_PS9,5_Spherical_Oriented_Top.npy\"              , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX2/bin2nm_doublepad/top'],\n",
        "         [\"PBX_PS9,5_Spherical_Oriented_Bottom_Mirrored.npy\"  , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX2/bin2nm_doublepad_mirror/btm'],\n",
        "         [\"PBX_PS9,5_Spherical_Oriented_Top_Mirrored.npy\"     , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX2/bin2nm_doublepad_mirror/top'],\n",
        "\n",
        "         [\"PBX_PS9,5_Spherical_Random_Bottom.npy\"           , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX3/bin2nm_doublepad/btm'],\n",
        "         [\"PBX_PS9,5_Spherical_Random_Top.npy\"              , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX3/bin2nm_doublepad/top'],\n",
        "         [\"PBX_PS9,5_Spherical_Random_Bottom_Mirrored.npy\"  , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX3/bin2nm_doublepad_mirror/btm'],\n",
        "         [\"PBX_PS9,5_Spherical_Random_Top_Mirrored.npy\"     , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX3/bin2nm_doublepad_mirror/top'],\n",
        "\n",
        "         [\"PBX_PS9,6_Faceted_Random_Bottom.npy\"           , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX4/bin2nm_doublepad/btm'],\n",
        "         [\"PBX_PS9,6_Faceted_Random_Top.npy\"              , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX4/bin2nm_doublepad/top'],\n",
        "         [\"PBX_PS9,6_Faceted_Random_Bottom_Mirrored.npy\"  , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX4/bin2nm_doublepad_mirror/btm'],\n",
        "         [\"PBX_PS9,6_Faceted_Random_Top_Mirrored.npy\"     , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX4/bin2nm_doublepad_mirror/top'],\n",
        "\n",
        "         [\"PBX_PS14,7_Spherical_Random_Bottom_Sim1.npy\"           , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX6/bin2nm_doublepad/btm'],\n",
        "         [\"PBX_PS14,7_Spherical_Random_Top_Sim1.npy\"              , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX6/bin2nm_doublepad/top'],\n",
        "         [\"PBX_PS14,7_Spherical_Random_Bottom_Mirrored_Sim1.npy\"  , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX6/bin2nm_doublepad_mirror/btm'],\n",
        "         [\"PBX_PS14,7_Spherical_Random_Top_Mirrored_Sim1.npy\"     , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX6/bin2nm_doublepad_mirror/top'],\n",
        "]\n",
        "\n",
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for i in paths:\n",
        "  train_ex = data_loader_inputs(i)\n",
        "  train_lb = data_loader_outputs(i)\n",
        "  train_data.append(train_ex)\n",
        "  train_labels.append(train_lb)\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "id": "gCb5lRG1mDbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d131eca9-b7f6-4b7d-d87c-a1d8cb07a1f7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 16, 34, 32, 3)\n",
            "(20, 16, 32, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION DATA\n",
        "\n",
        "validation_paths = [\n",
        "         [\"PBX_PS14,7_Spherical_Random_Bottom_Sim2.npy\"            , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX7/bin2nm_doublepad/btm'],\n",
        "         [\"PBX_PS14,7_Spherical_Random_Top_Sim2.npy\"               , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX7/bin2nm_doublepad/top'],\n",
        "         [\"PBX_PS14,7_Spherical_Random_Bottom_Mirrored_Sim2.npy\"   , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX7/bin2nm_doublepad_mirror/btm'],\n",
        "         [\"PBX_PS14,7_Spherical_Random_Top_Mirrored_Sim2.npy\"      , '/content/drive/MyDrive/PBX/PBXdatasets_small/smallPBX7/bin2nm_doublepad_mirror/top']\n",
        "]\n",
        "\n",
        "validation_data = []\n",
        "validation_labels = []\n",
        "\n",
        "for i in validation_paths:\n",
        "  validation_ex= data_loader_inputs(i)\n",
        "  validation_lb = data_loader_outputs(i)\n",
        "  validation_data.append(validation_ex)\n",
        "  validation_labels.append(validation_lb)\n",
        "\n",
        "validation_data = np.array(validation_data)\n",
        "validation_labels = np.array(validation_labels)\n",
        "\n",
        "print(validation_data.shape)\n",
        "print(validation_labels.shape)"
      ],
      "metadata": {
        "id": "02ORxH_vZNOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd993b9b-a09b-404d-b0a2-06471909860b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 16, 34, 32, 3)\n",
            "(4, 16, 32, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def periodic_padding_flexible(tensor, axis, padding=1):\n",
        "\n",
        "    if isinstance(axis,int):\n",
        "        axis = (axis,)\n",
        "    if isinstance(padding,int):\n",
        "        padding = (padding,)\n",
        "\n",
        "    ndim = len(tensor.shape)\n",
        "\n",
        "    for ax,p in zip(axis,padding):\n",
        "        # create a slice object that selects everything from all axes,\n",
        "        # except only 0:p for the specified for right, and -p: for left\n",
        "\n",
        "        ind_right = [slice(-p,None) if i == ax else slice(None) for i in range(ndim)]\n",
        "        ind_left = [slice(0, p) if i == ax else slice(None) for i in range(ndim)]\n",
        "        right = tensor[ind_right]\n",
        "        left = tensor[ind_left]\n",
        "        middle = tensor\n",
        "        tensor = tf.concat([right,middle,left], axis=ax)\n",
        "\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "fWvLsqQXsWGy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DownConvBlock(inputs, n_filters=32, filter_size = 3, max_pooling=True, special_padding=False):\n",
        "\n",
        "  padding_size = int((filter_size-1)/2)\n",
        "  kernel_init =   tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()\n",
        "\n",
        "\n",
        "  # PERIODIC PADDING\n",
        "\n",
        "  inputs = periodic_padding_flexible(inputs, axis=1,padding=padding_size)\n",
        "  if special_padding == False:\n",
        "    inputs = periodic_padding_flexible(inputs, axis=2,padding=padding_size)\n",
        "  inputs = periodic_padding_flexible(inputs, axis=3,padding=padding_size)\n",
        "\n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(inputs)\n",
        "  print(conv.shape)\n",
        "  conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "  conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv)\n",
        "  print(conv.shape)\n",
        "  conv = BatchNormalization()(conv, training=False)\n",
        "      \n",
        "  if max_pooling:\n",
        "    next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2))(conv)\n",
        "\n",
        "\n",
        "    # print(conv.shape)\n",
        "    # next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2), strides=(1,1,1))(conv)\n",
        "    # print(next_layer.shape)\n",
        "    # for i in range(int(conv.shape[1]/2) - 1):\n",
        "    #   next_layer = tf.keras.layers.MaxPooling3D(pool_size = (2,2,2), strides=(1,1,1))(next_layer)\n",
        "    #   print(next_layer.shape)\n",
        "    # for i in range(int(conv.shape[2]/4)):\n",
        "    #   next_layer = tf.keras.layers.MaxPooling3D(pool_size = (1,2,2), strides=(1,1,1))(next_layer)\n",
        "    #   print(next_layer.shape)\n",
        "\n",
        "  else:\n",
        "    next_layer = conv\n",
        "  \n",
        "  skip_connection = conv   \n",
        "\n",
        "  print(\"end_of_block\") \n",
        "  return next_layer, skip_connection"
      ],
      "metadata": {
        "id": "rLhZ3NIocWJ9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def UpConvBlock(prev_layer_input, skip_layer_input, filter_size = 3, n_filters=32):\n",
        "\n",
        "    padding_size = int((filter_size-1)/2)\n",
        "    kernel_init = tf.keras.initializers.GlorotUniform(seed=0)\n",
        "    bias_init = tf.keras.initializers.Zeros()   \n",
        "\n",
        "    up = Conv3DTranspose(n_filters, (filter_size,filter_size,filter_size),\n",
        "                         strides=(filter_size-1,filter_size-1,filter_size-1),\n",
        "                         padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init)(prev_layer_input)\n",
        "\n",
        "    merge = concatenate([up, skip_layer_input], axis=4)\n",
        "    merge = periodic_padding_flexible(merge, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(merge)\n",
        "    print(conv.shape)\n",
        "    conv = periodic_padding_flexible(conv, axis=(1,2,3),padding=(padding_size,padding_size,padding_size))\n",
        "    conv = Conv3D(n_filters, filter_size, activation='relu',padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv)\n",
        "    print(conv.shape)\n",
        "\n",
        "    print(\"end_of_block\")\n",
        "    return conv"
      ],
      "metadata": {
        "id": "YvMNFlsHseOF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def UNet3DModel(input_size=(16, 34, 32, 3), n_filters=32, filter_size=3, n_classes=1):\n",
        "  kernel_init =  tf.keras.initializers.GlorotUniform(seed=0)\n",
        "  bias_init = tf.keras.initializers.Zeros()  \n",
        "  \n",
        "  inputs = Input(input_size)\n",
        "  print(\"Inputs\", inputs.shape)\n",
        "\n",
        "  cblock0 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=False, special_padding=True)\n",
        "  print(\"CB0\", cblock0[0].shape)\n",
        "\n",
        "  cblock1 = DownConvBlock(inputs,     n_filters = n_filters    , filter_size = filter_size, max_pooling=True, special_padding=True)\n",
        "  print(\"CB1\", cblock1[0].shape)\n",
        "\n",
        "  cblock2 = DownConvBlock(cblock1[0], n_filters = n_filters*2  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB2\", cblock2[0].shape)\n",
        "    \n",
        "  cblock3 = DownConvBlock(cblock2[0], n_filters = n_filters*4  , filter_size = filter_size, max_pooling=True, special_padding=False)\n",
        "  print(\"CB3\", cblock3[0].shape)\n",
        "  \n",
        "  cblock4 = DownConvBlock(cblock3[0], n_filters = n_filters*8  , filter_size = filter_size, max_pooling=False, special_padding=False)\n",
        "  print(\"CB4\", cblock4[0].shape)\n",
        "\n",
        "\n",
        "  print(\"------------------\")\n",
        "\n",
        "  ublock7 = UpConvBlock(cblock4[0]   , cblock3[1],  n_filters = n_filters * 4, filter_size = filter_size)\n",
        "  print(\"UB7\", ublock7.shape)\n",
        "  \n",
        "  ublock8 = UpConvBlock(ublock7   , cblock2[1],  n_filters = n_filters * 2, filter_size = filter_size)\n",
        "  print(\"UB8\", ublock8.shape)\n",
        "  \n",
        "  ublock9 = UpConvBlock(ublock8   , cblock1[1],  n_filters = n_filters, filter_size = filter_size)\n",
        "  print(\"UB9\", ublock9.shape)\n",
        "\n",
        "  ublock9 = periodic_padding_flexible(ublock9, axis=(1,2,3),padding=(1,1,1))\n",
        "  \n",
        "  conv9 = Conv3D(n_filters, 3, activation='relu', padding='valid', kernel_initializer=kernel_init,  bias_initializer=bias_init)(ublock9)\n",
        "  print(\"C9\", conv9.shape)\n",
        "  \n",
        "  conv10 = Conv3D(n_classes, 1, padding='same', kernel_initializer=kernel_init,  bias_initializer=bias_init)(conv9)\n",
        "  print(\"C10\", conv10.shape)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=conv10)  \n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "fe-fUh99sgKg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def custom_mse(y_true,y_pred):\n",
        "    w_hot = 5.0\n",
        "    w_cold = 1.0\n",
        "    cutoff = 1.8\n",
        "    weightmat = tf.cast(tf.where(tf.greater(y_true, cutoff), w_hot, w_cold),float)\n",
        "    loss = tf.cast(K.square(y_pred - y_true),float)\n",
        "    loss = loss*weightmat\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "d2a5FVUYPHTB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet3DModel(input_size=(16, 34, 32, 3), n_filters=32, filter_size = 3, n_classes=1)\n",
        "optimizer = tf.optimizers.Adam(learning_rate = 0.0005)\n",
        "model.compile(loss=custom_mse, optimizer=optimizer, metrics=['mse'])"
      ],
      "metadata": {
        "id": "R3SJtgKps6tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d46f2ba7-a371-431a-9680-7de8b25453da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs (None, 16, 34, 32, 3)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB0 (None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "CB1 (None, 8, 16, 16, 32)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "CB2 (None, 4, 8, 8, 64)\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "CB3 (None, 2, 4, 4, 128)\n",
            "(None, 2, 4, 4, 256)\n",
            "(None, 2, 4, 4, 256)\n",
            "end_of_block\n",
            "CB4 (None, 2, 4, 4, 256)\n",
            "------------------\n",
            "(None, 4, 8, 8, 128)\n",
            "(None, 4, 8, 8, 128)\n",
            "end_of_block\n",
            "UB7 (None, 4, 8, 8, 128)\n",
            "(None, 8, 16, 16, 64)\n",
            "(None, 8, 16, 16, 64)\n",
            "end_of_block\n",
            "UB8 (None, 8, 16, 16, 64)\n",
            "(None, 16, 32, 32, 32)\n",
            "(None, 16, 32, 32, 32)\n",
            "end_of_block\n",
            "UB9 (None, 16, 32, 32, 32)\n",
            "C9 (None, 16, 32, 32, 32)\n",
            "C10 (None, 16, 32, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.summary()"
      ],
      "metadata": {
        "id": "n06aSBP25jGg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.001, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0.0005, patience=200, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True)\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch == 2000:\n",
        "    return lr /5\n",
        "  else:\n",
        "    return lr\n",
        "\n",
        "\n",
        "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=4000, validation_data=(validation_data, validation_labels), callbacks=[early, scheduler_cb])"
      ],
      "metadata": {
        "id": "baIEkfpntuOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3665671-1046-4091-8101-354c1cdf248e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4000\n",
            "1/1 [==============================] - 16s 16s/step - loss: 1.9605 - mse: 1.5329 - val_loss: 1.0022 - val_mse: 0.7952 - lr: 5.0000e-04\n",
            "Epoch 2/4000\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 1.1525 - mse: 0.8306 - val_loss: 0.7933 - val_mse: 0.6088 - lr: 5.0000e-04\n",
            "Epoch 3/4000\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.9220 - mse: 0.6346 - val_loss: 0.3383 - val_mse: 0.2128 - lr: 5.0000e-04\n",
            "Epoch 4/4000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.4212 - mse: 0.2253 - val_loss: 8.1474 - val_mse: 8.0337 - lr: 5.0000e-04\n",
            "Epoch 5/4000\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 8.4546 - mse: 8.2509 - val_loss: 0.2111 - val_mse: 0.1117 - lr: 5.0000e-04\n",
            "Epoch 6/4000\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.2758 - mse: 0.1206 - val_loss: 0.5538 - val_mse: 0.3976 - lr: 5.0000e-04\n",
            "Epoch 7/4000\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.6589 - mse: 0.4148 - val_loss: 0.6968 - val_mse: 0.5233 - lr: 5.0000e-04\n",
            "Epoch 8/4000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.8154 - mse: 0.5446 - val_loss: 0.7602 - val_mse: 0.5792 - lr: 5.0000e-04\n",
            "Epoch 9/4000\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.8846 - mse: 0.6026 - val_loss: 0.7934 - val_mse: 0.6086 - lr: 5.0000e-04\n",
            "Epoch 10/4000\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 0.9211 - mse: 0.6334 - val_loss: 0.8118 - val_mse: 0.6250 - lr: 5.0000e-04\n",
            "Epoch 11/4000\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.9418 - mse: 0.6508 - val_loss: 0.8219 - val_mse: 0.6339 - lr: 5.0000e-04\n",
            "Epoch 12/4000\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.9534 - mse: 0.6606 - val_loss: 0.8258 - val_mse: 0.6374 - lr: 5.0000e-04\n",
            "Epoch 13/4000\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 0.9582 - mse: 0.6647 - val_loss: 0.8241 - val_mse: 0.6359 - lr: 5.0000e-04\n",
            "Epoch 14/4000\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.9565 - mse: 0.6633 - val_loss: 0.8170 - val_mse: 0.6296 - lr: 5.0000e-04\n",
            "Epoch 15/4000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.9487 - mse: 0.6567 - val_loss: 0.8047 - val_mse: 0.6188 - lr: 5.0000e-04\n",
            "Epoch 16/4000\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 0.9352 - mse: 0.6454 - val_loss: 0.7876 - val_mse: 0.6036 - lr: 5.0000e-04\n",
            "Epoch 17/4000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.9164 - mse: 0.6294 - val_loss: 0.7653 - val_mse: 0.5839 - lr: 5.0000e-04\n",
            "Epoch 18/4000\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.8921 - mse: 0.6089 - val_loss: 0.7377 - val_mse: 0.5595 - lr: 5.0000e-04\n",
            "Epoch 19/4000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.8618 - mse: 0.5834 - val_loss: 0.7041 - val_mse: 0.5298 - lr: 5.0000e-04\n",
            "Epoch 20/4000\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.8251 - mse: 0.5525 - val_loss: 0.6636 - val_mse: 0.4941 - lr: 5.0000e-04\n",
            "Epoch 21/4000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.7807 - mse: 0.5154 - val_loss: 0.6154 - val_mse: 0.4518 - lr: 5.0000e-04\n",
            "Epoch 22/4000\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 0.7278 - mse: 0.4713 - val_loss: 0.5589 - val_mse: 0.4024 - lr: 5.0000e-04\n",
            "Epoch 23/4000\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.6656 - mse: 0.4199 - val_loss: 0.4931 - val_mse: 0.3453 - lr: 5.0000e-04\n",
            "Epoch 24/4000\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.5932 - mse: 0.3605 - val_loss: 0.4183 - val_mse: 0.2811 - lr: 5.0000e-04\n",
            "Epoch 25/4000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.5106 - mse: 0.2939 - val_loss: 0.3361 - val_mse: 0.2118 - lr: 5.0000e-04\n",
            "Epoch 26/4000\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.4196 - mse: 0.2224 - val_loss: 0.2489 - val_mse: 0.1410 - lr: 5.0000e-04\n",
            "Epoch 27/4000\n",
            "1/1 [==============================] - 0s 386ms/step - loss: 0.3223 - mse: 0.1500 - val_loss: 0.1685 - val_mse: 0.0842 - lr: 5.0000e-04\n",
            "Epoch 28/4000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.2298 - mse: 0.0938 - val_loss: 0.2473 - val_mse: 0.1998 - lr: 5.0000e-04\n",
            "Epoch 29/4000\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.2992 - mse: 0.2207 - val_loss: 0.2017 - val_mse: 0.1496 - lr: 5.0000e-04\n",
            "Epoch 30/4000\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.2576 - mse: 0.1705 - val_loss: 0.1826 - val_mse: 0.1279 - lr: 5.0000e-04\n",
            "Epoch 31/4000\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.2390 - mse: 0.1475 - val_loss: 0.1736 - val_mse: 0.1172 - lr: 5.0000e-04\n",
            "Epoch 32/4000\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.2313 - mse: 0.1364 - val_loss: 0.1695 - val_mse: 0.1117 - lr: 5.0000e-04\n",
            "Epoch 33/4000\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.2282 - mse: 0.1307 - val_loss: 0.1652 - val_mse: 0.1050 - lr: 5.0000e-04\n",
            "Epoch 34/4000\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 0.2245 - mse: 0.1233 - val_loss: 0.1594 - val_mse: 0.0951 - lr: 5.0000e-04\n",
            "Epoch 35/4000\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.2190 - mse: 0.1115 - val_loss: 0.1556 - val_mse: 0.0837 - lr: 5.0000e-04\n",
            "Epoch 36/4000\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.2157 - mse: 0.0970 - val_loss: 0.1596 - val_mse: 0.0815 - lr: 5.0000e-04\n",
            "Epoch 37/4000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.2209 - mse: 0.0929 - val_loss: 0.1585 - val_mse: 0.0812 - lr: 5.0000e-04\n",
            "Epoch 38/4000\n",
            "1/1 [==============================] - 0s 386ms/step - loss: 0.2196 - mse: 0.0927 - val_loss: 0.1549 - val_mse: 0.0819 - lr: 5.0000e-04\n",
            "Epoch 39/4000\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.2149 - mse: 0.0945 - val_loss: 0.1541 - val_mse: 0.0859 - lr: 5.0000e-04\n",
            "Epoch 40/4000\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.2133 - mse: 0.1001 - val_loss: 0.1545 - val_mse: 0.0890 - lr: 5.0000e-04\n",
            "Epoch 41/4000\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.2128 - mse: 0.1036 - val_loss: 0.1547 - val_mse: 0.0907 - lr: 5.0000e-04\n",
            "Epoch 42/4000\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.2123 - mse: 0.1054 - val_loss: 0.1543 - val_mse: 0.0908 - lr: 5.0000e-04\n",
            "Epoch 43/4000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.2114 - mse: 0.1053 - val_loss: 0.1525 - val_mse: 0.0882 - lr: 5.0000e-04\n",
            "Epoch 44/4000\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.2094 - mse: 0.1020 - val_loss: 0.1504 - val_mse: 0.0840 - lr: 5.0000e-04\n",
            "Epoch 45/4000\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.2074 - mse: 0.0969 - val_loss: 0.1491 - val_mse: 0.0808 - lr: 5.0000e-04\n",
            "Epoch 46/4000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.2064 - mse: 0.0929 - val_loss: 0.1485 - val_mse: 0.0790 - lr: 5.0000e-04\n",
            "Epoch 47/4000\n",
            "1/1 [==============================] - 0s 381ms/step - loss: 0.2059 - mse: 0.0907 - val_loss: 0.1480 - val_mse: 0.0788 - lr: 5.0000e-04\n",
            "Epoch 48/4000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.2050 - mse: 0.0904 - val_loss: 0.1479 - val_mse: 0.0805 - lr: 5.0000e-04\n",
            "Epoch 49/4000\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.2042 - mse: 0.0922 - val_loss: 0.1478 - val_mse: 0.0816 - lr: 5.0000e-04\n",
            "Epoch 50/4000\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.2034 - mse: 0.0934 - val_loss: 0.1472 - val_mse: 0.0811 - lr: 5.0000e-04\n",
            "Epoch 51/4000\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.2024 - mse: 0.0925 - val_loss: 0.1462 - val_mse: 0.0795 - lr: 5.0000e-04\n",
            "Epoch 52/4000\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.2012 - mse: 0.0905 - val_loss: 0.1453 - val_mse: 0.0782 - lr: 5.0000e-04\n",
            "Epoch 53/4000\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.2002 - mse: 0.0888 - val_loss: 0.1447 - val_mse: 0.0779 - lr: 5.0000e-04\n",
            "Epoch 54/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1991 - mse: 0.0882 - val_loss: 0.1442 - val_mse: 0.0785 - lr: 5.0000e-04\n",
            "Epoch 55/4000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.1978 - mse: 0.0886 - val_loss: 0.1439 - val_mse: 0.0795 - lr: 5.0000e-04\n",
            "Epoch 56/4000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.1965 - mse: 0.0893 - val_loss: 0.1437 - val_mse: 0.0805 - lr: 5.0000e-04\n",
            "Epoch 57/4000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.1953 - mse: 0.0899 - val_loss: 0.1437 - val_mse: 0.0810 - lr: 5.0000e-04\n",
            "Epoch 58/4000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.1944 - mse: 0.0900 - val_loss: 0.1426 - val_mse: 0.0790 - lr: 5.0000e-04\n",
            "Epoch 59/4000\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.1934 - mse: 0.0877 - val_loss: 0.1416 - val_mse: 0.0768 - lr: 5.0000e-04\n",
            "Epoch 60/4000\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.1928 - mse: 0.0853 - val_loss: 0.1410 - val_mse: 0.0761 - lr: 5.0000e-04\n",
            "Epoch 61/4000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.1921 - mse: 0.0843 - val_loss: 0.1409 - val_mse: 0.0770 - lr: 5.0000e-04\n",
            "Epoch 62/4000\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.1911 - mse: 0.0849 - val_loss: 0.1411 - val_mse: 0.0787 - lr: 5.0000e-04\n",
            "Epoch 63/4000\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.1901 - mse: 0.0862 - val_loss: 0.1409 - val_mse: 0.0795 - lr: 5.0000e-04\n",
            "Epoch 64/4000\n",
            "1/1 [==============================] - 0s 380ms/step - loss: 0.1891 - mse: 0.0866 - val_loss: 0.1401 - val_mse: 0.0787 - lr: 5.0000e-04\n",
            "Epoch 65/4000\n",
            "1/1 [==============================] - 0s 381ms/step - loss: 0.1881 - mse: 0.0855 - val_loss: 0.1392 - val_mse: 0.0776 - lr: 5.0000e-04\n",
            "Epoch 66/4000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.1872 - mse: 0.0842 - val_loss: 0.1386 - val_mse: 0.0771 - lr: 5.0000e-04\n",
            "Epoch 67/4000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.1863 - mse: 0.0835 - val_loss: 0.1384 - val_mse: 0.0775 - lr: 5.0000e-04\n",
            "Epoch 68/4000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.1854 - mse: 0.0836 - val_loss: 0.1383 - val_mse: 0.0783 - lr: 5.0000e-04\n",
            "Epoch 69/4000\n",
            "1/1 [==============================] - 0s 385ms/step - loss: 0.1845 - mse: 0.0839 - val_loss: 0.1381 - val_mse: 0.0788 - lr: 5.0000e-04\n",
            "Epoch 70/4000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.1835 - mse: 0.0839 - val_loss: 0.1377 - val_mse: 0.0787 - lr: 5.0000e-04\n",
            "Epoch 71/4000\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 0.1825 - mse: 0.0833 - val_loss: 0.1373 - val_mse: 0.0786 - lr: 5.0000e-04\n",
            "Epoch 72/4000\n",
            "1/1 [==============================] - 0s 385ms/step - loss: 0.1816 - mse: 0.0829 - val_loss: 0.1358 - val_mse: 0.0760 - lr: 5.0000e-04\n",
            "Epoch 73/4000\n",
            "1/1 [==============================] - 0s 382ms/step - loss: 0.1807 - mse: 0.0804 - val_loss: 0.1353 - val_mse: 0.0754 - lr: 5.0000e-04\n",
            "Epoch 74/4000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.1799 - mse: 0.0796 - val_loss: 0.1360 - val_mse: 0.0775 - lr: 5.0000e-04\n",
            "Epoch 75/4000\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 0.1789 - mse: 0.0808 - val_loss: 0.1369 - val_mse: 0.0797 - lr: 5.0000e-04\n",
            "Epoch 76/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1781 - mse: 0.0823 - val_loss: 0.1362 - val_mse: 0.0788 - lr: 5.0000e-04\n",
            "Epoch 77/4000\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.1772 - mse: 0.0812 - val_loss: 0.1350 - val_mse: 0.0767 - lr: 5.0000e-04\n",
            "Epoch 78/4000\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 0.1765 - mse: 0.0793 - val_loss: 0.1347 - val_mse: 0.0764 - lr: 5.0000e-04\n",
            "Epoch 79/4000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.1758 - mse: 0.0789 - val_loss: 0.1354 - val_mse: 0.0782 - lr: 5.0000e-04\n",
            "Epoch 80/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1751 - mse: 0.0800 - val_loss: 0.1358 - val_mse: 0.0795 - lr: 5.0000e-04\n",
            "Epoch 81/4000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.1744 - mse: 0.0807 - val_loss: 0.1350 - val_mse: 0.0784 - lr: 5.0000e-04\n",
            "Epoch 82/4000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.1736 - mse: 0.0796 - val_loss: 0.1343 - val_mse: 0.0773 - lr: 5.0000e-04\n",
            "Epoch 83/4000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.1730 - mse: 0.0785 - val_loss: 0.1346 - val_mse: 0.0783 - lr: 5.0000e-04\n",
            "Epoch 84/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1723 - mse: 0.0791 - val_loss: 0.1354 - val_mse: 0.0803 - lr: 5.0000e-04\n",
            "Epoch 85/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1717 - mse: 0.0803 - val_loss: 0.1351 - val_mse: 0.0801 - lr: 5.0000e-04\n",
            "Epoch 86/4000\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.1711 - mse: 0.0800 - val_loss: 0.1342 - val_mse: 0.0787 - lr: 5.0000e-04\n",
            "Epoch 87/4000\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.1706 - mse: 0.0788 - val_loss: 0.1342 - val_mse: 0.0791 - lr: 5.0000e-04\n",
            "Epoch 88/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1701 - mse: 0.0790 - val_loss: 0.1351 - val_mse: 0.0810 - lr: 5.0000e-04\n",
            "Epoch 89/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.1697 - mse: 0.0803 - val_loss: 0.1351 - val_mse: 0.0812 - lr: 5.0000e-04\n",
            "Epoch 90/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1693 - mse: 0.0804 - val_loss: 0.1344 - val_mse: 0.0800 - lr: 5.0000e-04\n",
            "Epoch 91/4000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.1689 - mse: 0.0795 - val_loss: 0.1343 - val_mse: 0.0801 - lr: 5.0000e-04\n",
            "Epoch 92/4000\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.1685 - mse: 0.0796 - val_loss: 0.1350 - val_mse: 0.0815 - lr: 5.0000e-04\n",
            "Epoch 93/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.1681 - mse: 0.0806 - val_loss: 0.1350 - val_mse: 0.0817 - lr: 5.0000e-04\n",
            "Epoch 94/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1678 - mse: 0.0809 - val_loss: 0.1344 - val_mse: 0.0806 - lr: 5.0000e-04\n",
            "Epoch 95/4000\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.1674 - mse: 0.0801 - val_loss: 0.1343 - val_mse: 0.0806 - lr: 5.0000e-04\n",
            "Epoch 96/4000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.1671 - mse: 0.0801 - val_loss: 0.1348 - val_mse: 0.0816 - lr: 5.0000e-04\n",
            "Epoch 97/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1667 - mse: 0.0810 - val_loss: 0.1345 - val_mse: 0.0813 - lr: 5.0000e-04\n",
            "Epoch 98/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1663 - mse: 0.0807 - val_loss: 0.1339 - val_mse: 0.0803 - lr: 5.0000e-04\n",
            "Epoch 99/4000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.1658 - mse: 0.0800 - val_loss: 0.1341 - val_mse: 0.0813 - lr: 5.0000e-04\n",
            "Epoch 100/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1652 - mse: 0.0808 - val_loss: 0.1340 - val_mse: 0.0817 - lr: 5.0000e-04\n",
            "Epoch 101/4000\n",
            "1/1 [==============================] - 0s 383ms/step - loss: 0.1644 - mse: 0.0812 - val_loss: 0.1319 - val_mse: 0.0781 - lr: 5.0000e-04\n",
            "Epoch 102/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.1639 - mse: 0.0782 - val_loss: 0.1393 - val_mse: 0.0918 - lr: 5.0000e-04\n",
            "Epoch 103/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1660 - mse: 0.0901 - val_loss: 0.1320 - val_mse: 0.0720 - lr: 5.0000e-04\n",
            "Epoch 104/4000\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 0.1682 - mse: 0.0741 - val_loss: 0.1311 - val_mse: 0.0729 - lr: 5.0000e-04\n",
            "Epoch 105/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.1658 - mse: 0.0746 - val_loss: 0.1348 - val_mse: 0.0845 - lr: 5.0000e-04\n",
            "Epoch 106/4000\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.1632 - mse: 0.0839 - val_loss: 0.1419 - val_mse: 0.0956 - lr: 5.0000e-04\n",
            "Epoch 107/4000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.1670 - mse: 0.0938 - val_loss: 0.1336 - val_mse: 0.0823 - lr: 5.0000e-04\n",
            "Epoch 108/4000\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.1625 - mse: 0.0822 - val_loss: 0.1308 - val_mse: 0.0738 - lr: 5.0000e-04\n",
            "Epoch 109/4000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.1637 - mse: 0.0752 - val_loss: 0.1308 - val_mse: 0.0728 - lr: 5.0000e-04\n",
            "Epoch 110/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1644 - mse: 0.0744 - val_loss: 0.1308 - val_mse: 0.0763 - lr: 5.0000e-04\n",
            "Epoch 111/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1617 - mse: 0.0771 - val_loss: 0.1348 - val_mse: 0.0852 - lr: 5.0000e-04\n",
            "Epoch 112/4000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.1619 - mse: 0.0849 - val_loss: 0.1366 - val_mse: 0.0883 - lr: 5.0000e-04\n",
            "Epoch 113/4000\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.1624 - mse: 0.0878 - val_loss: 0.1318 - val_mse: 0.0802 - lr: 5.0000e-04\n",
            "Epoch 114/4000\n",
            "1/1 [==============================] - 0s 391ms/step - loss: 0.1599 - mse: 0.0804 - val_loss: 0.1299 - val_mse: 0.0744 - lr: 5.0000e-04\n",
            "Epoch 115/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1605 - mse: 0.0754 - val_loss: 0.1298 - val_mse: 0.0745 - lr: 5.0000e-04\n",
            "Epoch 116/4000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.1599 - mse: 0.0753 - val_loss: 0.1313 - val_mse: 0.0804 - lr: 5.0000e-04\n",
            "Epoch 117/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1578 - mse: 0.0804 - val_loss: 0.1359 - val_mse: 0.0885 - lr: 5.0000e-04\n",
            "Epoch 118/4000\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.1593 - mse: 0.0878 - val_loss: 0.1320 - val_mse: 0.0811 - lr: 5.0000e-04\n",
            "Epoch 119/4000\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.1568 - mse: 0.0809 - val_loss: 0.1303 - val_mse: 0.0755 - lr: 5.0000e-04\n",
            "Epoch 120/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1572 - mse: 0.0760 - val_loss: 0.1305 - val_mse: 0.0755 - lr: 5.0000e-04\n",
            "Epoch 121/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1568 - mse: 0.0758 - val_loss: 0.1319 - val_mse: 0.0800 - lr: 5.0000e-04\n",
            "Epoch 122/4000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.1552 - mse: 0.0796 - val_loss: 0.1351 - val_mse: 0.0867 - lr: 5.0000e-04\n",
            "Epoch 123/4000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.1555 - mse: 0.0856 - val_loss: 0.1330 - val_mse: 0.0827 - lr: 5.0000e-04\n",
            "Epoch 124/4000\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.1537 - mse: 0.0813 - val_loss: 0.1321 - val_mse: 0.0788 - lr: 5.0000e-04\n",
            "Epoch 125/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1533 - mse: 0.0768 - val_loss: 0.1320 - val_mse: 0.0784 - lr: 5.0000e-04\n",
            "Epoch 126/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1527 - mse: 0.0763 - val_loss: 0.1329 - val_mse: 0.0818 - lr: 5.0000e-04\n",
            "Epoch 127/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.1515 - mse: 0.0795 - val_loss: 0.1342 - val_mse: 0.0850 - lr: 5.0000e-04\n",
            "Epoch 128/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1513 - mse: 0.0828 - val_loss: 0.1332 - val_mse: 0.0821 - lr: 5.0000e-04\n",
            "Epoch 129/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.1497 - mse: 0.0798 - val_loss: 0.1311 - val_mse: 0.0771 - lr: 5.0000e-04\n",
            "Epoch 130/4000\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.1488 - mse: 0.0751 - val_loss: 0.1328 - val_mse: 0.0805 - lr: 5.0000e-04\n",
            "Epoch 131/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.1480 - mse: 0.0771 - val_loss: 0.1347 - val_mse: 0.0837 - lr: 5.0000e-04\n",
            "Epoch 132/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1472 - mse: 0.0802 - val_loss: 0.1330 - val_mse: 0.0802 - lr: 5.0000e-04\n",
            "Epoch 133/4000\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.1447 - mse: 0.0763 - val_loss: 0.1351 - val_mse: 0.0815 - lr: 5.0000e-04\n",
            "Epoch 134/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1422 - mse: 0.0758 - val_loss: 0.1452 - val_mse: 0.0909 - lr: 5.0000e-04\n",
            "Epoch 135/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1412 - mse: 0.0798 - val_loss: 0.1402 - val_mse: 0.0813 - lr: 5.0000e-04\n",
            "Epoch 136/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1406 - mse: 0.0706 - val_loss: 0.1552 - val_mse: 0.1060 - lr: 5.0000e-04\n",
            "Epoch 137/4000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.1447 - mse: 0.0932 - val_loss: 0.1335 - val_mse: 0.0735 - lr: 5.0000e-04\n",
            "Epoch 138/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.1443 - mse: 0.0690 - val_loss: 0.1342 - val_mse: 0.0721 - lr: 5.0000e-04\n",
            "Epoch 139/4000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.1470 - mse: 0.0684 - val_loss: 0.1351 - val_mse: 0.0761 - lr: 5.0000e-04\n",
            "Epoch 140/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1402 - mse: 0.0705 - val_loss: 0.1489 - val_mse: 0.0962 - lr: 5.0000e-04\n",
            "Epoch 141/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1391 - mse: 0.0858 - val_loss: 0.1477 - val_mse: 0.0931 - lr: 5.0000e-04\n",
            "Epoch 142/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.1346 - mse: 0.0794 - val_loss: 0.1414 - val_mse: 0.0807 - lr: 5.0000e-04\n",
            "Epoch 143/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1374 - mse: 0.0691 - val_loss: 0.1436 - val_mse: 0.0869 - lr: 5.0000e-04\n",
            "Epoch 144/4000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.1315 - mse: 0.0725 - val_loss: 0.1554 - val_mse: 0.1028 - lr: 5.0000e-04\n",
            "Epoch 145/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1334 - mse: 0.0842 - val_loss: 0.1429 - val_mse: 0.0869 - lr: 5.0000e-04\n",
            "Epoch 146/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.1291 - mse: 0.0725 - val_loss: 0.1378 - val_mse: 0.0794 - lr: 5.0000e-04\n",
            "Epoch 147/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.1293 - mse: 0.0675 - val_loss: 0.1403 - val_mse: 0.0832 - lr: 5.0000e-04\n",
            "Epoch 148/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.1254 - mse: 0.0693 - val_loss: 0.1534 - val_mse: 0.0980 - lr: 5.0000e-04\n",
            "Epoch 149/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.1249 - mse: 0.0779 - val_loss: 0.1488 - val_mse: 0.0895 - lr: 5.0000e-04\n",
            "Epoch 150/4000\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.1211 - mse: 0.0691 - val_loss: 0.1399 - val_mse: 0.0817 - lr: 5.0000e-04\n",
            "Epoch 151/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.1200 - mse: 0.0655 - val_loss: 0.1455 - val_mse: 0.0906 - lr: 5.0000e-04\n",
            "Epoch 152/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.1169 - mse: 0.0706 - val_loss: 0.1504 - val_mse: 0.0933 - lr: 5.0000e-04\n",
            "Epoch 153/4000\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.1150 - mse: 0.0686 - val_loss: 0.1444 - val_mse: 0.0860 - lr: 5.0000e-04\n",
            "Epoch 154/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1117 - mse: 0.0633 - val_loss: 0.1441 - val_mse: 0.0879 - lr: 5.0000e-04\n",
            "Epoch 155/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.1113 - mse: 0.0660 - val_loss: 0.1465 - val_mse: 0.0870 - lr: 5.0000e-04\n",
            "Epoch 156/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.1088 - mse: 0.0615 - val_loss: 0.1533 - val_mse: 0.0983 - lr: 5.0000e-04\n",
            "Epoch 157/4000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.1081 - mse: 0.0693 - val_loss: 0.1409 - val_mse: 0.0816 - lr: 5.0000e-04\n",
            "Epoch 158/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.1060 - mse: 0.0580 - val_loss: 0.1484 - val_mse: 0.0911 - lr: 5.0000e-04\n",
            "Epoch 159/4000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.1036 - mse: 0.0631 - val_loss: 0.1532 - val_mse: 0.0937 - lr: 5.0000e-04\n",
            "Epoch 160/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.1012 - mse: 0.0612 - val_loss: 0.1469 - val_mse: 0.0900 - lr: 5.0000e-04\n",
            "Epoch 161/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0996 - mse: 0.0609 - val_loss: 0.1426 - val_mse: 0.0861 - lr: 5.0000e-04\n",
            "Epoch 162/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0975 - mse: 0.0588 - val_loss: 0.1480 - val_mse: 0.0889 - lr: 5.0000e-04\n",
            "Epoch 163/4000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0958 - mse: 0.0573 - val_loss: 0.1527 - val_mse: 0.0958 - lr: 5.0000e-04\n",
            "Epoch 164/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0939 - mse: 0.0606 - val_loss: 0.1421 - val_mse: 0.0815 - lr: 5.0000e-04\n",
            "Epoch 165/4000\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.0970 - mse: 0.0533 - val_loss: 0.1606 - val_mse: 0.1113 - lr: 5.0000e-04\n",
            "Epoch 166/4000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.1084 - mse: 0.0807 - val_loss: 0.1373 - val_mse: 0.0723 - lr: 5.0000e-04\n",
            "Epoch 167/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.1175 - mse: 0.0548 - val_loss: 0.1410 - val_mse: 0.0759 - lr: 5.0000e-04\n",
            "Epoch 168/4000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.1093 - mse: 0.0536 - val_loss: 0.1554 - val_mse: 0.0999 - lr: 5.0000e-04\n",
            "Epoch 169/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.1018 - mse: 0.0676 - val_loss: 0.1590 - val_mse: 0.1078 - lr: 5.0000e-04\n",
            "Epoch 170/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.1002 - mse: 0.0711 - val_loss: 0.1389 - val_mse: 0.0816 - lr: 5.0000e-04\n",
            "Epoch 171/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0936 - mse: 0.0532 - val_loss: 0.1414 - val_mse: 0.0801 - lr: 5.0000e-04\n",
            "Epoch 172/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0993 - mse: 0.0529 - val_loss: 0.1465 - val_mse: 0.0888 - lr: 5.0000e-04\n",
            "Epoch 173/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0921 - mse: 0.0558 - val_loss: 0.1539 - val_mse: 0.1021 - lr: 5.0000e-04\n",
            "Epoch 174/4000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0916 - mse: 0.0642 - val_loss: 0.1450 - val_mse: 0.0907 - lr: 5.0000e-04\n",
            "Epoch 175/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0888 - mse: 0.0564 - val_loss: 0.1429 - val_mse: 0.0866 - lr: 5.0000e-04\n",
            "Epoch 176/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0871 - mse: 0.0533 - val_loss: 0.1460 - val_mse: 0.0894 - lr: 5.0000e-04\n",
            "Epoch 177/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.0858 - mse: 0.0543 - val_loss: 0.1478 - val_mse: 0.0902 - lr: 5.0000e-04\n",
            "Epoch 178/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0849 - mse: 0.0537 - val_loss: 0.1468 - val_mse: 0.0885 - lr: 5.0000e-04\n",
            "Epoch 179/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0829 - mse: 0.0513 - val_loss: 0.1456 - val_mse: 0.0892 - lr: 5.0000e-04\n",
            "Epoch 180/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0808 - mse: 0.0510 - val_loss: 0.1478 - val_mse: 0.0945 - lr: 5.0000e-04\n",
            "Epoch 181/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.0805 - mse: 0.0542 - val_loss: 0.1486 - val_mse: 0.0937 - lr: 5.0000e-04\n",
            "Epoch 182/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0783 - mse: 0.0518 - val_loss: 0.1509 - val_mse: 0.0927 - lr: 5.0000e-04\n",
            "Epoch 183/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0765 - mse: 0.0492 - val_loss: 0.1525 - val_mse: 0.0945 - lr: 5.0000e-04\n",
            "Epoch 184/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0755 - mse: 0.0497 - val_loss: 0.1492 - val_mse: 0.0917 - lr: 5.0000e-04\n",
            "Epoch 185/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0744 - mse: 0.0484 - val_loss: 0.1470 - val_mse: 0.0897 - lr: 5.0000e-04\n",
            "Epoch 186/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0730 - mse: 0.0469 - val_loss: 0.1519 - val_mse: 0.0951 - lr: 5.0000e-04\n",
            "Epoch 187/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0711 - mse: 0.0477 - val_loss: 0.1572 - val_mse: 0.0993 - lr: 5.0000e-04\n",
            "Epoch 188/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0702 - mse: 0.0478 - val_loss: 0.1539 - val_mse: 0.0962 - lr: 5.0000e-04\n",
            "Epoch 189/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0685 - mse: 0.0456 - val_loss: 0.1539 - val_mse: 0.0985 - lr: 5.0000e-04\n",
            "Epoch 190/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0677 - mse: 0.0467 - val_loss: 0.1540 - val_mse: 0.0954 - lr: 5.0000e-04\n",
            "Epoch 191/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0664 - mse: 0.0434 - val_loss: 0.1634 - val_mse: 0.1051 - lr: 5.0000e-04\n",
            "Epoch 192/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0656 - mse: 0.0458 - val_loss: 0.1574 - val_mse: 0.0976 - lr: 5.0000e-04\n",
            "Epoch 193/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0641 - mse: 0.0421 - val_loss: 0.1626 - val_mse: 0.1065 - lr: 5.0000e-04\n",
            "Epoch 194/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0637 - mse: 0.0460 - val_loss: 0.1582 - val_mse: 0.0961 - lr: 5.0000e-04\n",
            "Epoch 195/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0643 - mse: 0.0408 - val_loss: 0.1710 - val_mse: 0.1171 - lr: 5.0000e-04\n",
            "Epoch 196/4000\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 0.0665 - mse: 0.0507 - val_loss: 0.1502 - val_mse: 0.0890 - lr: 5.0000e-04\n",
            "Epoch 197/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0667 - mse: 0.0392 - val_loss: 0.1567 - val_mse: 0.0989 - lr: 5.0000e-04\n",
            "Epoch 198/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0596 - mse: 0.0401 - val_loss: 0.1737 - val_mse: 0.1194 - lr: 5.0000e-04\n",
            "Epoch 199/4000\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.0646 - mse: 0.0495 - val_loss: 0.1554 - val_mse: 0.0936 - lr: 5.0000e-04\n",
            "Epoch 200/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0619 - mse: 0.0380 - val_loss: 0.1556 - val_mse: 0.0974 - lr: 5.0000e-04\n",
            "Epoch 201/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.0578 - mse: 0.0387 - val_loss: 0.1719 - val_mse: 0.1175 - lr: 5.0000e-04\n",
            "Epoch 202/4000\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.0601 - mse: 0.0469 - val_loss: 0.1669 - val_mse: 0.1037 - lr: 5.0000e-04\n",
            "Epoch 203/4000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.0586 - mse: 0.0394 - val_loss: 0.1577 - val_mse: 0.0984 - lr: 5.0000e-04\n",
            "Epoch 204/4000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0550 - mse: 0.0367 - val_loss: 0.1627 - val_mse: 0.1102 - lr: 5.0000e-04\n",
            "Epoch 205/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0574 - mse: 0.0441 - val_loss: 0.1637 - val_mse: 0.1048 - lr: 5.0000e-04\n",
            "Epoch 206/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0523 - mse: 0.0369 - val_loss: 0.1694 - val_mse: 0.1058 - lr: 5.0000e-04\n",
            "Epoch 207/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0543 - mse: 0.0373 - val_loss: 0.1707 - val_mse: 0.1128 - lr: 5.0000e-04\n",
            "Epoch 208/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0515 - mse: 0.0392 - val_loss: 0.1641 - val_mse: 0.1072 - lr: 5.0000e-04\n",
            "Epoch 209/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0510 - mse: 0.0380 - val_loss: 0.1631 - val_mse: 0.1017 - lr: 5.0000e-04\n",
            "Epoch 210/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0502 - mse: 0.0343 - val_loss: 0.1742 - val_mse: 0.1144 - lr: 5.0000e-04\n",
            "Epoch 211/4000\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.0494 - mse: 0.0368 - val_loss: 0.1679 - val_mse: 0.1097 - lr: 5.0000e-04\n",
            "Epoch 212/4000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0473 - mse: 0.0353 - val_loss: 0.1609 - val_mse: 0.1017 - lr: 5.0000e-04\n",
            "Epoch 213/4000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0477 - mse: 0.0333 - val_loss: 0.1695 - val_mse: 0.1103 - lr: 5.0000e-04\n",
            "Epoch 214/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0454 - mse: 0.0340 - val_loss: 0.1774 - val_mse: 0.1167 - lr: 5.0000e-04\n",
            "Epoch 215/4000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0455 - mse: 0.0347 - val_loss: 0.1687 - val_mse: 0.1071 - lr: 5.0000e-04\n",
            "Epoch 216/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0443 - mse: 0.0319 - val_loss: 0.1695 - val_mse: 0.1117 - lr: 5.0000e-04\n",
            "Epoch 217/4000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0442 - mse: 0.0343 - val_loss: 0.1712 - val_mse: 0.1095 - lr: 5.0000e-04\n",
            "Epoch 218/4000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0423 - mse: 0.0313 - val_loss: 0.1760 - val_mse: 0.1136 - lr: 5.0000e-04\n",
            "Epoch 219/4000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0420 - mse: 0.0316 - val_loss: 0.1742 - val_mse: 0.1147 - lr: 5.0000e-04\n",
            "Epoch 220/4000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0410 - mse: 0.0322 - val_loss: 0.1687 - val_mse: 0.1062 - lr: 5.0000e-04\n",
            "Epoch 221/4000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0411 - mse: 0.0295 - val_loss: 0.1808 - val_mse: 0.1205 - lr: 5.0000e-04\n",
            "Epoch 222/4000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0406 - mse: 0.0323 - val_loss: 0.1728 - val_mse: 0.1090 - lr: 5.0000e-04\n",
            "Epoch 223/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0401 - mse: 0.0289 - val_loss: 0.1739 - val_mse: 0.1165 - lr: 5.0000e-04\n",
            "Epoch 224/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0395 - mse: 0.0319 - val_loss: 0.1699 - val_mse: 0.1071 - lr: 5.0000e-04\n",
            "Epoch 225/4000\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.0387 - mse: 0.0279 - val_loss: 0.1797 - val_mse: 0.1193 - lr: 5.0000e-04\n",
            "Epoch 226/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0370 - mse: 0.0297 - val_loss: 0.1765 - val_mse: 0.1140 - lr: 5.0000e-04\n",
            "Epoch 227/4000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0363 - mse: 0.0277 - val_loss: 0.1770 - val_mse: 0.1154 - lr: 5.0000e-04\n",
            "Epoch 228/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0355 - mse: 0.0279 - val_loss: 0.1756 - val_mse: 0.1135 - lr: 5.0000e-04\n",
            "Epoch 229/4000\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.0348 - mse: 0.0270 - val_loss: 0.1775 - val_mse: 0.1159 - lr: 5.0000e-04\n",
            "Epoch 230/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0341 - mse: 0.0269 - val_loss: 0.1800 - val_mse: 0.1178 - lr: 5.0000e-04\n",
            "Epoch 231/4000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0337 - mse: 0.0266 - val_loss: 0.1782 - val_mse: 0.1153 - lr: 5.0000e-04\n",
            "Epoch 232/4000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0333 - mse: 0.0259 - val_loss: 0.1820 - val_mse: 0.1206 - lr: 5.0000e-04\n",
            "Epoch 233/4000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0326 - mse: 0.0266 - val_loss: 0.1778 - val_mse: 0.1133 - lr: 5.0000e-04\n",
            "Epoch 234/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0335 - mse: 0.0249 - val_loss: 0.1910 - val_mse: 0.1344 - lr: 5.0000e-04\n",
            "Epoch 235/4000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0397 - mse: 0.0337 - val_loss: 0.1775 - val_mse: 0.1038 - lr: 5.0000e-04\n",
            "Epoch 236/4000\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.0589 - mse: 0.0360 - val_loss: 0.1810 - val_mse: 0.1226 - lr: 5.0000e-04\n",
            "Epoch 237/4000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0377 - mse: 0.0303 - val_loss: 0.1790 - val_mse: 0.1264 - lr: 5.0000e-04\n",
            "Epoch 238/4000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0437 - mse: 0.0374 - val_loss: 0.1597 - val_mse: 0.0940 - lr: 5.0000e-04\n",
            "Epoch 239/4000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0456 - mse: 0.0271 - val_loss: 0.1677 - val_mse: 0.1014 - lr: 5.0000e-04\n",
            "Epoch 240/4000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0418 - mse: 0.0276 - val_loss: 0.1843 - val_mse: 0.1271 - lr: 5.0000e-04\n",
            "Epoch 241/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0431 - mse: 0.0347 - val_loss: 0.1711 - val_mse: 0.1128 - lr: 5.0000e-04\n",
            "Epoch 242/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0354 - mse: 0.0285 - val_loss: 0.1649 - val_mse: 0.1014 - lr: 5.0000e-04\n",
            "Epoch 243/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0389 - mse: 0.0264 - val_loss: 0.1699 - val_mse: 0.1081 - lr: 5.0000e-04\n",
            "Epoch 244/4000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0352 - mse: 0.0256 - val_loss: 0.1802 - val_mse: 0.1219 - lr: 5.0000e-04\n",
            "Epoch 245/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0353 - mse: 0.0292 - val_loss: 0.1744 - val_mse: 0.1134 - lr: 5.0000e-04\n",
            "Epoch 246/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0346 - mse: 0.0268 - val_loss: 0.1691 - val_mse: 0.1050 - lr: 5.0000e-04\n",
            "Epoch 247/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0337 - mse: 0.0243 - val_loss: 0.1743 - val_mse: 0.1118 - lr: 5.0000e-04\n",
            "Epoch 248/4000\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.0317 - mse: 0.0244 - val_loss: 0.1838 - val_mse: 0.1235 - lr: 5.0000e-04\n",
            "Epoch 249/4000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0328 - mse: 0.0275 - val_loss: 0.1817 - val_mse: 0.1167 - lr: 5.0000e-04\n",
            "Epoch 250/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0313 - mse: 0.0249 - val_loss: 0.1775 - val_mse: 0.1126 - lr: 5.0000e-04\n",
            "Epoch 251/4000\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0305 - mse: 0.0238 - val_loss: 0.1757 - val_mse: 0.1143 - lr: 5.0000e-04\n",
            "Epoch 252/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0302 - mse: 0.0245 - val_loss: 0.1768 - val_mse: 0.1151 - lr: 5.0000e-04\n",
            "Epoch 253/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0292 - mse: 0.0236 - val_loss: 0.1784 - val_mse: 0.1134 - lr: 5.0000e-04\n",
            "Epoch 254/4000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0289 - mse: 0.0222 - val_loss: 0.1810 - val_mse: 0.1171 - lr: 5.0000e-04\n",
            "Epoch 255/4000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0281 - mse: 0.0226 - val_loss: 0.1818 - val_mse: 0.1206 - lr: 5.0000e-04\n",
            "Epoch 256/4000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0280 - mse: 0.0233 - val_loss: 0.1775 - val_mse: 0.1148 - lr: 5.0000e-04\n",
            "Epoch 257/4000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0270 - mse: 0.0216 - val_loss: 0.1770 - val_mse: 0.1123 - lr: 5.0000e-04\n",
            "Epoch 258/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0269 - mse: 0.0210 - val_loss: 0.1807 - val_mse: 0.1173 - lr: 5.0000e-04\n",
            "Epoch 259/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0265 - mse: 0.0216 - val_loss: 0.1819 - val_mse: 0.1191 - lr: 5.0000e-04\n",
            "Epoch 260/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0258 - mse: 0.0216 - val_loss: 0.1785 - val_mse: 0.1136 - lr: 5.0000e-04\n",
            "Epoch 261/4000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0255 - mse: 0.0203 - val_loss: 0.1783 - val_mse: 0.1139 - lr: 5.0000e-04\n",
            "Epoch 262/4000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0250 - mse: 0.0202 - val_loss: 0.1806 - val_mse: 0.1178 - lr: 5.0000e-04\n",
            "Epoch 263/4000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0248 - mse: 0.0208 - val_loss: 0.1799 - val_mse: 0.1154 - lr: 5.0000e-04\n",
            "Epoch 264/4000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0244 - mse: 0.0199 - val_loss: 0.1800 - val_mse: 0.1146 - lr: 5.0000e-04\n",
            "Epoch 265/4000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0239 - mse: 0.0194 - val_loss: 0.1818 - val_mse: 0.1177 - lr: 5.0000e-04\n",
            "Epoch 266/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0236 - mse: 0.0198 - val_loss: 0.1810 - val_mse: 0.1164 - lr: 5.0000e-04\n",
            "Epoch 267/4000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0232 - mse: 0.0193 - val_loss: 0.1793 - val_mse: 0.1140 - lr: 5.0000e-04\n",
            "Epoch 268/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0230 - mse: 0.0188 - val_loss: 0.1811 - val_mse: 0.1172 - lr: 5.0000e-04\n",
            "Epoch 269/4000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0225 - mse: 0.0189 - val_loss: 0.1812 - val_mse: 0.1163 - lr: 5.0000e-04\n",
            "Epoch 270/4000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0223 - mse: 0.0184 - val_loss: 0.1812 - val_mse: 0.1158 - lr: 5.0000e-04\n",
            "Epoch 271/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0220 - mse: 0.0181 - val_loss: 0.1821 - val_mse: 0.1184 - lr: 5.0000e-04\n",
            "Epoch 272/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0216 - mse: 0.0183 - val_loss: 0.1803 - val_mse: 0.1162 - lr: 5.0000e-04\n",
            "Epoch 273/4000\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.0214 - mse: 0.0178 - val_loss: 0.1799 - val_mse: 0.1153 - lr: 5.0000e-04\n",
            "Epoch 274/4000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0210 - mse: 0.0174 - val_loss: 0.1827 - val_mse: 0.1184 - lr: 5.0000e-04\n",
            "Epoch 275/4000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0208 - mse: 0.0177 - val_loss: 0.1825 - val_mse: 0.1171 - lr: 5.0000e-04\n",
            "Epoch 276/4000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0205 - mse: 0.0171 - val_loss: 0.1809 - val_mse: 0.1161 - lr: 5.0000e-04\n",
            "Epoch 277/4000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0202 - mse: 0.0169 - val_loss: 0.1813 - val_mse: 0.1171 - lr: 5.0000e-04\n",
            "Epoch 278/4000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0200 - mse: 0.0170 - val_loss: 0.1824 - val_mse: 0.1167 - lr: 5.0000e-04\n",
            "Epoch 279/4000\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 0.0198 - mse: 0.0165 - val_loss: 0.1830 - val_mse: 0.1184 - lr: 5.0000e-04\n",
            "Epoch 280/4000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0195 - mse: 0.0166 - val_loss: 0.1816 - val_mse: 0.1165 - lr: 5.0000e-04\n",
            "Epoch 281/4000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0192 - mse: 0.0161 - val_loss: 0.1828 - val_mse: 0.1177 - lr: 5.0000e-04\n",
            "Epoch 282/4000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0189 - mse: 0.0161 - val_loss: 0.1836 - val_mse: 0.1188 - lr: 5.0000e-04\n",
            "Epoch 283/4000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0187 - mse: 0.0161 - val_loss: 0.1822 - val_mse: 0.1166 - lr: 5.0000e-04\n",
            "Epoch 284/4000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0185 - mse: 0.0156 - val_loss: 0.1823 - val_mse: 0.1181 - lr: 5.0000e-04\n",
            "Epoch 285/4000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0184 - mse: 0.0158 - val_loss: 0.1812 - val_mse: 0.1155 - lr: 5.0000e-04\n",
            "Epoch 286/4000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0182 - mse: 0.0152 - val_loss: 0.1837 - val_mse: 0.1196 - lr: 5.0000e-04\n",
            "Epoch 287/4000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0181 - mse: 0.0157 - val_loss: 0.1827 - val_mse: 0.1157 - lr: 5.0000e-04\n",
            "Epoch 288/4000\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.0181 - mse: 0.0150 - val_loss: 0.1848 - val_mse: 0.1215 - lr: 5.0000e-04\n",
            "Epoch 289/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0182 - mse: 0.0161 - val_loss: 0.1822 - val_mse: 0.1137 - lr: 5.0000e-04\n",
            "Epoch 290/4000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0190 - mse: 0.0152 - val_loss: 0.1857 - val_mse: 0.1246 - lr: 5.0000e-04\n",
            "Epoch 291/4000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0197 - mse: 0.0176 - val_loss: 0.1822 - val_mse: 0.1120 - lr: 5.0000e-04\n",
            "Epoch 292/4000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0210 - mse: 0.0161 - val_loss: 0.1851 - val_mse: 0.1236 - lr: 5.0000e-04\n",
            "Epoch 293/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0191 - mse: 0.0170 - val_loss: 0.1798 - val_mse: 0.1137 - lr: 5.0000e-04\n",
            "Epoch 294/4000\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 0.0171 - mse: 0.0141 - val_loss: 0.1817 - val_mse: 0.1151 - lr: 5.0000e-04\n",
            "Epoch 295/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0171 - mse: 0.0142 - val_loss: 0.1852 - val_mse: 0.1233 - lr: 5.0000e-04\n",
            "Epoch 296/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0182 - mse: 0.0163 - val_loss: 0.1815 - val_mse: 0.1123 - lr: 5.0000e-04\n",
            "Epoch 297/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0187 - mse: 0.0148 - val_loss: 0.1839 - val_mse: 0.1200 - lr: 5.0000e-04\n",
            "Epoch 298/4000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0167 - mse: 0.0147 - val_loss: 0.1816 - val_mse: 0.1179 - lr: 5.0000e-04\n",
            "Epoch 299/4000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0163 - mse: 0.0143 - val_loss: 0.1806 - val_mse: 0.1122 - lr: 5.0000e-04\n",
            "Epoch 300/4000\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 0.0174 - mse: 0.0139 - val_loss: 0.1863 - val_mse: 0.1229 - lr: 5.0000e-04\n",
            "Epoch 301/4000\n",
            "1/1 [==============================] - 0s 371ms/step - loss: 0.0170 - mse: 0.0151 - val_loss: 0.1814 - val_mse: 0.1151 - lr: 5.0000e-04\n",
            "Epoch 302/4000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0158 - mse: 0.0132 - val_loss: 0.1805 - val_mse: 0.1146 - lr: 5.0000e-04\n",
            "Epoch 303/4000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0157 - mse: 0.0132 - val_loss: 0.1874 - val_mse: 0.1233 - lr: 5.0000e-04\n",
            "Epoch 304/4000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0161 - mse: 0.0143 - val_loss: 0.1831 - val_mse: 0.1151 - lr: 5.0000e-04\n",
            "Epoch 305/4000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0159 - mse: 0.0132 - val_loss: 0.1815 - val_mse: 0.1175 - lr: 5.0000e-04\n",
            "Epoch 306/4000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0156 - mse: 0.0136 - val_loss: 0.1850 - val_mse: 0.1188 - lr: 5.0000e-04\n",
            "Epoch 307/4000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0150 - mse: 0.0130 - val_loss: 0.1841 - val_mse: 0.1171 - lr: 5.0000e-04\n",
            "Epoch 308/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0151 - mse: 0.0130 - val_loss: 0.1836 - val_mse: 0.1192 - lr: 5.0000e-04\n",
            "Epoch 309/4000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0152 - mse: 0.0134 - val_loss: 0.1833 - val_mse: 0.1157 - lr: 5.0000e-04\n",
            "Epoch 310/4000\n",
            "1/1 [==============================] - 0s 371ms/step - loss: 0.0148 - mse: 0.0125 - val_loss: 0.1861 - val_mse: 0.1208 - lr: 5.0000e-04\n",
            "Epoch 311/4000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0147 - mse: 0.0130 - val_loss: 0.1846 - val_mse: 0.1179 - lr: 5.0000e-04\n",
            "Epoch 312/4000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0144 - mse: 0.0124 - val_loss: 0.1823 - val_mse: 0.1164 - lr: 5.0000e-04\n",
            "Epoch 313/4000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0142 - mse: 0.0123 - val_loss: 0.1868 - val_mse: 0.1207 - lr: 5.0000e-04\n",
            "Epoch 314/4000\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0142 - mse: 0.0125 - val_loss: 0.1848 - val_mse: 0.1173 - lr: 5.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())\n",
        "# # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "#plt.ylim([0,0.2])\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UgCp0oNUAfzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def area_hotspot(datapoint):\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  tinc = 5\n",
        "  tnum = int(4500/tinc)\n",
        "\n",
        "  bins=np.zeros((tnum,2))\n",
        "  bins[:,0] = [ tinc*(0.5 + x) for x in list(range(tnum))]\n",
        "\n",
        "  for bin_index, temp in np.ndenumerate(datapoint):\n",
        "    theta = temp * 1000\n",
        "    tind=math.floor(theta/tinc)\n",
        "    bins[:tind, 1] += 4 # ~ Roughly 2*2*1 nm^3 (volume value from Chunyu)\n",
        "  \n",
        "  return bins\n",
        "\n",
        "def plot_prediction(data_to_predict, labels_for_data):\n",
        "\n",
        "  maxval = max(np.max(data_to_predict), np.max(labels_for_data))\n",
        "  colorscale = [[0, 'rgba(0,0,0,0)'], [0.5, 'rgba(0,0,0,0)'], [1, 'rgb(255,0,0)']]\n",
        "\n",
        "  fig = make_subplots(rows=2, cols=3, specs=[[{\"type\": \"scatter3d\"}, {\"type\": \"histogram\"}, {\"type\": \"histogram\"}],[{\"type\": \"scatter3d\"}, {\"type\": \"scatter\"},{\"type\": \"scatter\"}]],\n",
        "    subplot_titles=['Prediction (Temperature)',  'Difference', 'Temp Distribution',  'Labels (Temperature)', 'Parity', 'Volume Plot'], horizontal_spacing = 0.15, vertical_spacing = 0.15)\n",
        "  \n",
        "\n",
        "  # FIRST PLOT --> PREDICTIONS\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:data_to_predict.shape[2], 0:data_to_predict.shape[1], 0:data_to_predict.shape[0]]\n",
        "  data_to_predict_xz = np.swapaxes(data_to_predict, 2, 0)\n",
        "\n",
        "  trace_1 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(), hovertemplate = 'Bin: (%{x},%{y},%{z})<br>Temperature: %{marker.color:.2f}<extra></extra>',\n",
        "                          mode='markers', marker=dict(colorscale=colorscale, symbol='square', color = data_to_predict_xz.flatten(), line=dict(width=0.5, color='rgba(0,0,0,0.025)'),\n",
        "                                                      cmin=0, cmax=maxval, colorbar=dict(thickness=20, len=0.3, x = 0.27, y= 0.8)), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "  ####\n",
        "\n",
        "  # SECOND PLOT --> LABELS\n",
        "\n",
        "  X_t,Y_t,Z_t = np.mgrid[0:labels_for_data.shape[2], 0:labels_for_data.shape[1], 0:labels_for_data.shape[0]]\n",
        "  labels_for_data_xz = np.swapaxes(labels_for_data, 2, 0)\n",
        "  trace_2 = go.Scatter3d(x = X_t.flatten(), y = Y_t.flatten(), z=Z_t.flatten(), hovertemplate = 'Bin: (%{x},%{y},%{z})<br>Temperature: %{marker.color:.2f}<extra></extra>',\n",
        "                          mode='markers', marker=dict(colorscale=colorscale, symbol='square', color = labels_for_data_xz.flatten(), line=dict(width=0.5, color='rgba(0,0,0,0.025)'),\n",
        "                                                      cmin=0, cmax=maxval, colorbar=dict(thickness=20,  len=0.3, x = 0.27, y= 0.3)),showlegend=False)\n",
        "  fig.add_trace(trace_2, row=2, col=1)\n",
        "  \n",
        "  ####\n",
        "\n",
        "  # TRIRD PLOT\n",
        "  diff_xz = data_to_predict_xz - labels_for_data_xz\n",
        "  flat_diff_xz = diff_xz.flatten()\n",
        "\n",
        "  trace_3 = go.Histogram(x=flat_diff_xz, showlegend=False)\n",
        "  trace_line = go.Scatter(x=[0,0], y = [0,700], mode='lines', showlegend=False)\n",
        "\n",
        "  fig.add_trace(trace_3, row=1, col=2)\n",
        "  fig.add_trace(trace_line, row=1, col=2)\n",
        "\n",
        "\n",
        "  # FOURTH PLOT\n",
        "  trace_4 = go.Scatter(x=labels_for_data_xz.flatten(), y = data_to_predict_xz.flatten(), mode='markers', showlegend=False)\n",
        "  trace_math = go.Scatter(x=[0,maxval], y=[0, maxval], mode='lines', showlegend=False)\n",
        "  fig.add_trace(trace_4, row=2, col=2)\n",
        "  fig.add_trace(trace_math, row=2,col=2)\n",
        "  fig.update_xaxes(title=\"Scaled Temperature (K) - Labels\", row=2, col=2)\n",
        "  fig.update_yaxes(title=\"Scaled Temperature (K) - Predictions\", row=2, col=2)\n",
        "\n",
        "  # FIFTH PLOT\n",
        "  trace_5 = go.Histogram(x=data_to_predict_xz.flatten(), name='Predictions', marker=dict(color='red'), showlegend=True)\n",
        "  trace_6 = go.Histogram(x=labels_for_data_xz.flatten(), name='Truth', marker=dict(color='green'), showlegend=True)\n",
        "  fig.update_xaxes(title=\"Temperature (K) - Labels\", row=1, col=3)\n",
        "  fig.add_trace(trace_5, row=1, col=3)\n",
        "  fig.add_trace(trace_6, row=1, col=3) \n",
        "\n",
        "  # SIXTH PLOT\n",
        "  \n",
        "  bins_labels = area_hotspot(labels_for_data)\n",
        "  bins_predictions = area_hotspot(data_to_predict)\n",
        "  trace_7 = go.Scatter(x=bins_predictions[:,1], y=bins_predictions[:,0], mode='lines', marker=dict(color='red'), showlegend=False)\n",
        "  trace_8 = go.Scatter(x=bins_labels[:,1], y=bins_labels[:,0], mode='lines', marker=dict(color='green'), showlegend=False)\n",
        "\n",
        "\n",
        "  fig.add_trace(trace_7, row=2, col=3)\n",
        "  fig.add_trace(trace_8, row=2, col=3) \n",
        "  fig.update_xaxes(type=\"log\", row=2, col=3)\n",
        "  fig.update_xaxes(title=\"Hotspot Volume (nm^3)\", row=2, col=3)\n",
        "  fig.update_yaxes(title=\"Temperature (K)\", row=2, col=3)  \n",
        "\n",
        "  fig.update_layout(autosize=False, width=1200, height=800, margin=dict(l=75, r=75, b=75, t=75), legend=dict(x=0.85, y=0.25))      \n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "iTwnIzqkErW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TRAINING DATA\n",
        "\n",
        "pred_training_data = model.predict(train_data)\n",
        "#print(pred_training_data.shape)\n",
        "\n",
        "labels_training_data = train_labels.copy()\n",
        "#print(labels_training_data.shape)\n"
      ],
      "metadata": {
        "id": "VhI-HUEWwIRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in [9]:#pred_training_data.shape[0]):\n",
        "  training_cube = pred_training_data[i].squeeze()\n",
        "  training_label = labels_training_data[i].squeeze()\n",
        "  plot_prediction(training_cube, training_label)\n",
        "  print(\" \")\n"
      ],
      "metadata": {
        "id": "VtaBemKOQbWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### VALIDATION DATA\n",
        "\n",
        "pred_validation_data = model.predict(validation_data)\n",
        "#print(pred_validation_data.shape)\n",
        "\n",
        "labels_validation_data = validation_labels.copy()\n",
        "#print(labels_validation_data.shape)\n"
      ],
      "metadata": {
        "id": "HQ7JU8A7d6lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [0]:#pred_validation_data.shape[0]):\n",
        "\n",
        "  validation_cube = pred_validation_data[i].squeeze()\n",
        "  validation_label = labels_validation_data[i].squeeze()\n",
        "  plot_prediction(validation_cube, validation_label)"
      ],
      "metadata": {
        "id": "QYEvIqocyL7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING DATA\n",
        "\n",
        "test_paths = ['/content/drive/MyDrive/PBX/PBXdatasets_small/testPBX6/btm',\n",
        "              '/content/drive/MyDrive/PBX/PBXdatasets_small/testPBX6/top']\n",
        "\n",
        "test_data = []\n",
        "test_labels = []\n",
        "\n",
        "for i in test_paths:\n",
        "  test_ex= data_loader_inputs(i)\n",
        "  test_data.append(test_ex)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "e-KhOBQZfoAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_only_prediction(data_to_predict, title):\n",
        "\n",
        "\n",
        "  maxval = max([np.max(data_to_predict)])\n",
        "\n",
        "  fig = make_subplots(rows=1, cols=1, specs=[[{\"type\": \"scatter3d\"}]], subplot_titles=[title], horizontal_spacing = 0.15, vertical_spacing = 0.15)\n",
        "  \n",
        "\n",
        "  # FIRST PLOT --> PREDICTIONS\n",
        "\n",
        "  X,Y,Z = np.mgrid[0:data_to_predict.shape[2], 0:data_to_predict.shape[1], 0:data_to_predict.shape[0]]\n",
        "  data_to_predict_xz = np.swapaxes(data_to_predict, 2, 0)\n",
        "\n",
        "  trace_1 = go.Scatter3d(x = X.flatten(), y = Y.flatten(), z=Z.flatten(),\n",
        "                          mode='markers', marker=dict(colorscale='Viridis', symbol='square', color = data_to_predict_xz.flatten(),\n",
        "                                                      cmin=0, cmax=maxval, colorbar=dict(thickness=20)), showlegend=False)\n",
        "  fig.add_trace(trace_1, row=1, col=1)\n",
        "  #### \n",
        "\n",
        "  fig.update_layout(autosize=False, width=800, height=600, legend=dict(x=0.85, y=0.25))      \n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "GtjH1Pxf2-EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_testing_data = model.predict(test_data)\n",
        "#print(pred_testing_data.shape)\n",
        "\n",
        "\n",
        "for i in [0]:#pred_testing_data.shape[0]):\n",
        "\n",
        "  testing_cube = pred_testing_data[i].squeeze()\n",
        "  plot_only_prediction(testing_cube, 'Prediction (Temperature)')"
      ],
      "metadata": {
        "id": "Px40Hid33dEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [0]:#pred_testing_data.shape[0]):\n",
        "\n",
        "  input1_cube = test_data[i,:,:,:,0].squeeze()\n",
        "  plot_only_prediction(input1_cube, 'HE Density')\n",
        "\n",
        "  input2_cube = test_data[i,:,:,:,1].squeeze()\n",
        "  plot_only_prediction(input2_cube, 'Total Density')\n",
        "\n",
        "  input3_cube = test_data[i,:,:,:,2].squeeze()\n",
        "  plot_only_prediction(input3_cube, 'GB Interface')"
      ],
      "metadata": {
        "id": "-BR8XhSM4F3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhHN27p35vWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}